{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入所需库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据及数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial missing timestamps: 48\n",
      "Dealing with missing data.\n",
      "Initial missing dates: 2025-02-13    24\n",
      "2025-02-12    23\n",
      "2025-02-14     1\n",
      "Name: count, dtype: int64\n",
      "Missing timestamps per day:\n",
      "date\n",
      "2024-01-01     0\n",
      "2024-01-02     0\n",
      "2024-01-03     0\n",
      "2024-01-04     0\n",
      "2024-01-05     0\n",
      "              ..\n",
      "2025-02-11     0\n",
      "2025-02-12    23\n",
      "2025-02-13    24\n",
      "2025-02-14     1\n",
      "2025-02-15     0\n",
      "Length: 412, dtype: int64\n",
      "\n",
      "Days to drop: 2\n",
      "Total missing timestamps: 48\n",
      "\n",
      "Missing timestamps by date:\n",
      "2025-02-12    24\n",
      "2025-02-13    24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "data_path = '../data/'\n",
    "weather_data = pd.read_csv(data_path + 'neimeng_weather.csv', sep=',')\n",
    "solar_data = pd.read_csv(data_path + 'neimeng_solar_output.csv', sep=',')\n",
    "wind_data = pd.read_csv(data_path + 'neimeng_wind_output.csv', sep=',')\n",
    "\n",
    "weather_data.drop(columns=['id', 'lng', 'lat', 'region_name'], inplace=True)\n",
    "solar_data.drop(columns=['type'], inplace=True)\n",
    "wind_data.drop(columns=['type'], inplace=True)\n",
    "\n",
    "city_code_map = {\n",
    "    '乌兰察布':\t1509,\n",
    "    '锡林郭勒':\t1525,\n",
    "    '包头':\t1502,\n",
    "    '巴彦淖尔':\t1508,\n",
    "    '阿拉善盟':\t1529,\n",
    "    '呼和浩特':\t1501,\n",
    "    '鄂尔多斯':\t1506,\n",
    "    '乌海':\t1503,\n",
    "    '鄂尔多斯+薛家湾': 1506\n",
    "}\n",
    "\n",
    "solar_data['city_name'] = solar_data['city_name'].map(city_code_map)\n",
    "wind_data['city_name'] = wind_data['city_name'].map(city_code_map)\n",
    "weather_data.rename(columns={'region_code': 'city_code', 'ts': 'datetime'}, inplace=True)\n",
    "solar_data.rename(columns={'city_name': 'city_code', 'date_time': 'datetime', 'value': 'solar_output'}, inplace=True)\n",
    "wind_data.rename(columns={'city_name': 'city_code', 'date_time': 'datetime', 'value': 'wind_output'}, inplace=True)\n",
    "\n",
    "weather_data['datetime'] = pd.to_datetime(weather_data['datetime'], dayfirst=True)\n",
    "weather_data.set_index(weather_data['datetime'], inplace=True)\n",
    "weather_data.drop(columns=['datetime'], inplace=True)\n",
    "\n",
    "solar_data['datetime'] = pd.to_datetime(solar_data['datetime'], dayfirst=True)\n",
    "solar_data.set_index(solar_data['datetime'], inplace=True)\n",
    "solar_data.drop(columns=['datetime'], inplace=True)\n",
    "\n",
    "wind_data['datetime'] = pd.to_datetime(wind_data['datetime'], dayfirst=True)\n",
    "wind_data.set_index(wind_data['datetime'], inplace=True)\n",
    "wind_data.drop(columns=['datetime'], inplace=True)\n",
    "\n",
    "target_city_code = 1508\n",
    "mask = weather_data['city_code'] == target_city_code\n",
    "weather_data = weather_data[mask]\n",
    "mask = solar_data['city_code'] == target_city_code\n",
    "solar_data = solar_data[mask]\n",
    "mask = wind_data['city_code'] == target_city_code\n",
    "wind_data = wind_data[mask]\n",
    "\n",
    "solar_data = solar_data.resample('h', closed='right', label='right').mean()\n",
    "wind_data = wind_data.resample('h', closed='right', label='right').mean()\n",
    "\n",
    "merged_df = pd.concat([weather_data, solar_data, wind_data], axis=1)\n",
    "merged_df.dropna(inplace=True)\n",
    "merged_df.drop(columns=['city_code'], inplace=True)\n",
    "\n",
    "def deal_missing_data(df_data):\n",
    "    complete_time_range = pd.date_range(\n",
    "        start=df_data.index.min(),\n",
    "        end=df_data.index.max(),\n",
    "        freq='h'\n",
    "    )\n",
    "    initial_missing = complete_time_range.difference(df_data.index)\n",
    "    print(f\"Initial missing timestamps: {len(initial_missing)}\")\n",
    "    if len(initial_missing) == 0:\n",
    "        print(\"No need to deal missing data.\")\n",
    "        return df_data\n",
    "\n",
    "    print(\"Dealing with missing data.\")\n",
    "    print(\"Initial missing dates:\", pd.Series(initial_missing.date).value_counts())\n",
    "\n",
    "    df_reindexed = df_data.reindex(complete_time_range)\n",
    "    df_reindexed['date'] = df_reindexed.index.date\n",
    "    missing_flag = df_reindexed.drop(columns='date').isna().any(axis=1)\n",
    "    missing_count_per_day = missing_flag.groupby(df_reindexed['date']).sum()\n",
    "    print(\"Missing timestamps per day:\")\n",
    "    print(missing_count_per_day)\n",
    "\n",
    "    threshold = 8\n",
    "    days_to_drop = missing_count_per_day[missing_count_per_day > threshold].index\n",
    "    print(f\"\\nDays to drop: {len(days_to_drop)}\")\n",
    "\n",
    "    df_cleaned = df_reindexed[~df_reindexed['date'].isin(days_to_drop)].copy()\n",
    "    df_cleaned.drop(columns='date', inplace=True)\n",
    "    \n",
    "    df_filled = df_cleaned.interpolate(method='time')\n",
    "\n",
    "    expected_range = pd.date_range(\n",
    "        start=df_filled.index.min(),\n",
    "        end=df_filled.index.max(),\n",
    "        freq='h'\n",
    "    )\n",
    "    missing_timestamps = expected_range.difference(df_filled.index)\n",
    "    print(f\"Total missing timestamps: {len(missing_timestamps)}\")\n",
    "    print(\"\\nMissing timestamps by date:\")\n",
    "    missing_dates = pd.Series(missing_timestamps.date).value_counts().sort_index()\n",
    "    print(missing_dates)\n",
    "    \n",
    "    return df_filled\n",
    "\n",
    "df_cleaned = deal_missing_data(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalized:\n",
      "                        t2m   ws10m  ws100m      ssrd   tp      rh  \\\n",
      "2024-01-01 11:00:00 -5.3400  5.4026  7.4081  286.9511  0.0  0.5627   \n",
      "2024-01-01 12:00:00 -4.8384  5.8687  7.7351  388.4978  0.0  0.5411   \n",
      "2024-01-01 13:00:00 -0.0111  6.0763  7.9036  439.6089  0.0  0.4029   \n",
      "2024-01-01 14:00:00  0.2727  6.1327  7.9847  432.9600  0.0  0.3896   \n",
      "2024-01-01 15:00:00  0.5780  5.9767  7.8884  367.9467  0.0  0.3798   \n",
      "\n",
      "                     solar_output  wind_output  \n",
      "2024-01-01 11:00:00       946.277     2549.727  \n",
      "2024-01-01 12:00:00      1294.318     2402.862  \n",
      "2024-01-01 13:00:00      1230.975     2602.524  \n",
      "2024-01-01 14:00:00      1227.397     2706.338  \n",
      "2024-01-01 15:00:00      1165.689     2813.120  \n",
      "\n",
      "After normalized:\n",
      "                          t2m     ws10m    ws100m      ssrd        tp  \\\n",
      "2024-01-01 11:00:00 -0.953372  1.382216  0.904447  0.385923 -0.106259   \n",
      "2024-01-01 12:00:00 -0.917707  1.656826  1.023772  0.775450 -0.106259   \n",
      "2024-01-01 13:00:00 -0.574475  1.779136  1.085258  0.971509 -0.106259   \n",
      "2024-01-01 14:00:00 -0.554297  1.812365  1.114852  0.946004 -0.106259   \n",
      "2024-01-01 15:00:00 -0.532589  1.720455  1.079712  0.696617 -0.106259   \n",
      "\n",
      "                           rh  solar_output  wind_output            datetime  \n",
      "2024-01-01 11:00:00  0.412373      0.375125     0.654412 2024-01-01 11:00:00  \n",
      "2024-01-01 12:00:00  0.313709      0.513096     0.616717 2024-01-01 12:00:00  \n",
      "2024-01-01 13:00:00 -0.317556      0.487986     0.667963 2024-01-01 13:00:00  \n",
      "2024-01-01 14:00:00 -0.378308      0.486567     0.694608 2024-01-01 14:00:00  \n",
      "2024-01-01 15:00:00 -0.423072      0.462105     0.722014 2024-01-01 15:00:00  \n"
     ]
    }
   ],
   "source": [
    "print(\"Before normalized:\")\n",
    "print(df_cleaned.iloc[10:15])\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "columns_to_scale = [col for col in df_cleaned.columns]\n",
    "normalized_df = df_cleaned.copy()\n",
    "\n",
    "for col in columns_to_scale:\n",
    "    if 'output' in col:\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    normalized_df[col] = scaler.fit_transform(normalized_df[[col]])\n",
    "    scalers[col] = scaler\n",
    "\n",
    "with open('../model/scalers.pkl', 'wb') as f:\n",
    "    pickle.dump(scalers, f)\n",
    "\n",
    "normalized_df['datetime'] = normalized_df.index\n",
    "\n",
    "print(\"\\nAfter normalized:\")\n",
    "print(normalized_df.iloc[10:15])\n",
    "\n",
    "normalized_df.to_csv(\"../output/normalized_df.csv\", index=True, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "规范化数据验证:\n",
      "总行数: 9816\n",
      "缺失值统计:\n",
      "t2m             0\n",
      "ws10m           0\n",
      "ws100m          0\n",
      "ssrd            0\n",
      "tp              0\n",
      "rh              0\n",
      "solar_output    0\n",
      "wind_output     0\n",
      "datetime        0\n",
      "dtype: int64\n",
      "\n",
      "数值范围:\n",
      "               t2m         ws10m        ws100m          ssrd           tp  \\\n",
      "count  9816.000000  9.816000e+03  9.816000e+03  9.816000e+03  9816.000000   \n",
      "mean      0.000000 -3.691695e-16 -4.082581e-16  1.737268e-16     0.000000   \n",
      "min      -2.391871 -1.788015e+00 -1.780387e+00 -7.148030e-01    -0.106259   \n",
      "25%      -0.901206 -7.064284e-01 -8.008784e-01 -7.148030e-01    -0.106259   \n",
      "50%       0.056376 -2.293824e-01 -1.159584e-01 -6.960155e-01    -0.106259   \n",
      "75%       0.868079  5.554870e-01  6.987222e-01  6.488299e-01    -0.106259   \n",
      "max       2.109066  5.078630e+00  4.381639e+00  3.033913e+00    49.624781   \n",
      "std       1.000051  1.000051e+00  1.000051e+00  1.000051e+00     1.000051   \n",
      "\n",
      "                 rh  solar_output  wind_output                       datetime  \n",
      "count  9.816000e+03   9816.000000  9816.000000                           9816  \n",
      "mean   1.563541e-16      0.190863     0.343179  2024-07-23 12:37:20.097799680  \n",
      "min   -1.937743e+00      0.000000     0.000000            2024-01-01 01:00:00  \n",
      "25%   -7.830118e-01      0.000662     0.108332            2024-04-12 06:45:00  \n",
      "50%   -9.236541e-02      0.007364     0.293700            2024-07-23 12:30:00  \n",
      "75%    7.029970e-01      0.379630     0.546642            2024-11-02 18:15:00  \n",
      "max    2.410314e+00      1.000000     1.000000            2025-02-15 00:00:00  \n",
      "std    1.000051e+00      0.262540     0.260749                            NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df.columns\n",
    "\n",
    "# 数据验证函数\n",
    "def validate_dataset(df, name=\"数据集\"):\n",
    "    \"\"\"验证数据集的完整性\"\"\"\n",
    "    print(f\"\\n{name}验证:\")\n",
    "    print(f\"总行数: {len(df)}\")\n",
    "    print(f\"缺失值统计:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(f\"\\n数值范围:\")\n",
    "    print(df.describe())\n",
    "    return not df.isnull().any().any()\n",
    "\n",
    "# 在创建数据集之前进行验证\n",
    "validate_dataset(normalized_df, \"规范化数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/TFT/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/opt/anaconda3/envs/TFT/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "规范化数据验证:\n",
      "总行数: 9816\n",
      "缺失值统计:\n",
      "t2m             0\n",
      "ws10m           0\n",
      "ws100m          0\n",
      "ssrd            0\n",
      "tp              0\n",
      "rh              0\n",
      "solar_output    0\n",
      "wind_output     0\n",
      "datetime        0\n",
      "time_idx        0\n",
      "group_id        0\n",
      "month           0\n",
      "dtype: int64\n",
      "\n",
      "数值范围:\n",
      "               t2m         ws10m        ws100m          ssrd           tp  \\\n",
      "count  9816.000000  9.816000e+03  9.816000e+03  9.816000e+03  9816.000000   \n",
      "mean      0.000000 -3.691695e-16 -4.082581e-16  1.737268e-16     0.000000   \n",
      "min      -2.391871 -1.788015e+00 -1.780387e+00 -7.148030e-01    -0.106259   \n",
      "25%      -0.901206 -7.064284e-01 -8.008784e-01 -7.148030e-01    -0.106259   \n",
      "50%       0.056376 -2.293824e-01 -1.159584e-01 -6.960155e-01    -0.106259   \n",
      "75%       0.868079  5.554870e-01  6.987222e-01  6.488299e-01    -0.106259   \n",
      "max       2.109066  5.078630e+00  4.381639e+00  3.033913e+00    49.624781   \n",
      "std       1.000051  1.000051e+00  1.000051e+00  1.000051e+00     1.000051   \n",
      "\n",
      "                 rh  solar_output  wind_output                       datetime  \\\n",
      "count  9.816000e+03   9816.000000  9816.000000                           9816   \n",
      "mean   1.563541e-16      0.190863     0.343179  2024-07-23 12:37:20.097799680   \n",
      "min   -1.937743e+00      0.000000     0.000000            2024-01-01 01:00:00   \n",
      "25%   -7.830118e-01      0.000662     0.108332            2024-04-12 06:45:00   \n",
      "50%   -9.236541e-02      0.007364     0.293700            2024-07-23 12:30:00   \n",
      "75%    7.029970e-01      0.379630     0.546642            2024-11-02 18:15:00   \n",
      "max    2.410314e+00      1.000000     1.000000            2025-02-15 00:00:00   \n",
      "std    1.000051e+00      0.262540     0.260749                            NaN   \n",
      "\n",
      "          time_idx  \n",
      "count  9816.000000  \n",
      "mean   4908.500000  \n",
      "min       1.000000  \n",
      "25%    2454.750000  \n",
      "50%    4908.500000  \n",
      "75%    7362.250000  \n",
      "max    9816.000000  \n",
      "std    2833.779455  \n",
      "\n",
      "长格式数据验证:\n",
      "总行数: 19632\n",
      "缺失值统计:\n",
      "datetime       0\n",
      "time_idx       0\n",
      "t2m            0\n",
      "ws10m          0\n",
      "ws100m         0\n",
      "ssrd           0\n",
      "tp             0\n",
      "rh             0\n",
      "output_type    0\n",
      "target         0\n",
      "dtype: int64\n",
      "\n",
      "数值范围:\n",
      "                            datetime      time_idx           t2m  \\\n",
      "count                          19632  19632.000000  1.963200e+04   \n",
      "mean   2024-07-23 12:37:20.097799680   4908.500000 -2.316358e-17   \n",
      "min              2024-01-01 01:00:00      1.000000 -2.391871e+00   \n",
      "25%              2024-04-12 06:45:00   2454.750000 -9.012063e-01   \n",
      "50%              2024-07-23 12:30:00   4908.500000  5.637586e-02   \n",
      "75%              2024-11-02 18:15:00   7362.250000  8.680793e-01   \n",
      "max              2025-02-15 00:00:00   9816.000000  2.109066e+00   \n",
      "std                              NaN   2833.707278  1.000025e+00   \n",
      "\n",
      "              ws10m        ws100m          ssrd            tp            rh  \\\n",
      "count  1.963200e+04  1.963200e+04  1.963200e+04  1.963200e+04  1.963200e+04   \n",
      "mean  -3.612070e-16 -4.039149e-16  1.737268e-16 -5.790894e-18  1.621450e-16   \n",
      "min   -1.788015e+00 -1.780387e+00 -7.148030e-01 -1.062589e-01 -1.937743e+00   \n",
      "25%   -7.064284e-01 -8.008784e-01 -7.148030e-01 -1.062589e-01 -7.830118e-01   \n",
      "50%   -2.293824e-01 -1.159584e-01 -6.960155e-01 -1.062589e-01 -9.236541e-02   \n",
      "75%    5.554870e-01  6.987222e-01  6.488299e-01 -1.062589e-01  7.029970e-01   \n",
      "max    5.078630e+00  4.381639e+00  3.033913e+00  4.962478e+01  2.410314e+00   \n",
      "std    1.000025e+00  1.000025e+00  1.000025e+00  1.000025e+00  1.000025e+00   \n",
      "\n",
      "             target  \n",
      "count  19632.000000  \n",
      "mean       0.267021  \n",
      "min        0.000000  \n",
      "25%        0.004686  \n",
      "50%        0.174369  \n",
      "75%        0.492047  \n",
      "max        1.000000  \n",
      "std        0.272498  \n",
      "\n",
      "数据集信息:\n",
      "训练集大小: 9781\n",
      "验证集大小: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/TFT/lib/python3.9/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/opt/anaconda3/envs/TFT/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 73     | train\n",
      "3  | prescalers                         | ModuleDict                      | 384    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.8 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 18.7 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 16.5 K | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 8.4 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 8.4 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 231    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "94.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "94.5 K    Total params\n",
      "0.378     Total estimated model params size (MB)\n",
      "463       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b752e13d2674356b00ee6ca6b136a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/TFT/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24b232ea18a43fb94595d47e8db9f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9492ae4ae1a4579af042a1494020bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487aa5483a07490e84202c81e22b59e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0001. New best score: 0.002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25e93a259234e52993c41c14caf681b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0001. New best score: 0.002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93dbe1c27f7845b7aaf770083f06a341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0001. New best score: 0.002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf90d19802d043d4819db908d6bc0512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be3317ff55041209e2d18f8d8b52ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7133c7307cc140cd869042d153aae15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb9b2ec75434852848bd88c0a0265ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ee229bc9714d65be1c1dcbd0adf3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c726f6650aa48288c9f5a52417bb119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0001. New best score: 0.001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10accf57aca04eb2a42dc2b1dcde3120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b853262999d49068a9e5c93accfde80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9509565c2b534dcd998d22e70d883521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c8d1965251442f80b3c84f9c69b34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec541fa0ed44b439bcc534eabd74a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334403b357ec4314a3278dec926559c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0616f1811c46410390c7b5ab35e799b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cdef65ac134ed791ffac79f7c4cc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0001. New best score: 0.001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e822ac6776f4977a614f5c9f1b9328b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae33a60261347a1ab658d2e41d80a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18706314eb64ccba27599b2c5869a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdec85f24b843889b6b83d73564cf81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ff0f2dcc9b4654aa6e0ed9656631d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51875544b88c4e97b6e7fefb0c3c53c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97d40bb82a342279855e7cd29f5613a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c252614a25434d0f9e04bc7a25fbeb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc0001b93964e3689911ac042569935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e643fa464154d45b261d636ba3e891c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 10 records. Best score: 0.001. Signaling Trainer to stop.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "模型训练完成\n",
      "预测过程中发生错误: too many values to unpack (expected 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 150\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mis_global_zero:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m         raw_predictions, x \u001b[38;5;241m=\u001b[39m tft\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    151\u001b[0m             val_dataloader,\n\u001b[1;32m    152\u001b[0m             mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    153\u001b[0m             return_x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    154\u001b[0m             trainer_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccelerator\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    155\u001b[0m         )\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;66;03m# 输出预测结果\u001b[39;00m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m预测完成\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# 数据预处理验证\n",
    "if normalized_df.isnull().any().any():\n",
    "    print(\"警告：数据中存在缺失值\")\n",
    "    normalized_df = normalized_df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# 创建时间索引\n",
    "normalized_df['time_idx'] = np.arange(1, len(normalized_df) + 1)\n",
    "normalized_df['group_id'] = '0'\n",
    "normalized_df['month'] = normalized_df['datetime'].dt.month.astype(str)\n",
    "\n",
    "# 数据重构为长格式\n",
    "df_long = normalized_df.melt(\n",
    "    id_vars=['datetime', 'time_idx', 't2m', 'ws10m', 'ws100m', 'ssrd', 'tp', 'rh'],\n",
    "    value_vars=['solar_output', 'wind_output'],\n",
    "    var_name='output_type',\n",
    "    value_name='target'\n",
    ")\n",
    "\n",
    "# 数据验证函数\n",
    "def validate_dataset(df, name=\"数据集\"):\n",
    "    \"\"\"验证数据集的完整性\"\"\"\n",
    "    print(f\"\\n{name}验证:\")\n",
    "    print(f\"总行数: {len(df)}\")\n",
    "    print(f\"缺失值统计:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(f\"\\n数值范围:\")\n",
    "    print(df.describe())\n",
    "    return not df.isnull().any().any()\n",
    "\n",
    "# 在创建数据集之前进行验证\n",
    "validate_dataset(normalized_df, \"规范化数据\")\n",
    "validate_dataset(df_long, \"长格式数据\")\n",
    "\n",
    "# 模型参数设置\n",
    "max_encoder_length = 24   # 历史输入长度（例如过去24个时间步）\n",
    "max_prediction_length = 6 # 预测未来6个时间步\n",
    "\n",
    "# 为了保证每个时间序列足够长，需要保证：min_encoder_length + min_prediction_length <= 序列总长度\n",
    "min_encoder_length = max_encoder_length  # 此处固定为 max_encoder_length\n",
    "min_prediction_length = max_prediction_length\n",
    "\n",
    "training_cutoff = df_long[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "# 数据集划分\n",
    "training_data = df_long[df_long[\"time_idx\"] <= training_cutoff].copy()\n",
    "\n",
    "# 创建训练数据集\n",
    "training_dataset = TimeSeriesDataSet(\n",
    "    normalized_df[normalized_df['time_idx'] <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target='solar_output',\n",
    "    group_ids=[\"group_id\"],\n",
    "    min_encoder_length=min_encoder_length,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=min_prediction_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"group_id\"],\n",
    "    time_varying_known_categoricals=[\"month\"],\n",
    "    time_varying_known_reals=[\"time_idx\", \"t2m\", \"ws10m\", \"ws100m\", \"ssrd\", \"tp\", \"rh\"],\n",
    "    time_varying_unknown_reals=[\"solar_output\"],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"group_id\"],\n",
    "        transformation=\"softplus\"\n",
    "    ),\n",
    "    add_relative_time_idx=True,\n",
    "    add_encoder_length=True,\n",
    "    add_target_scales=True,      # 添加目标尺度信息（用于逆归一化）\n",
    "    allow_missing_timesteps=True  # 允许缺失时间步\n",
    ")\n",
    "\n",
    "# 创建验证数据集\n",
    "validation_dataset = TimeSeriesDataSet.from_dataset(\n",
    "    training_dataset, \n",
    "    normalized_df, \n",
    "    predict=True, \n",
    "    stop_randomization=True\n",
    ")\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 64\n",
    "train_dataloader = training_dataset.to_dataloader(\n",
    "    train=True, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=7,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_dataloader = validation_dataset.to_dataloader(\n",
    "    train=False, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=7,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# 添加数据验证步骤\n",
    "print(\"\\n数据集信息:\")\n",
    "print(f\"训练集大小: {len(training_dataset)}\")\n",
    "print(f\"验证集大小: {len(validation_dataset)}\")\n",
    "\n",
    "# 创建TFT模型\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training_dataset,\n",
    "    learning_rate=0.01,          \n",
    "    hidden_size=32,              \n",
    "    attention_head_size=2,       \n",
    "    dropout=0.2,                 \n",
    "    hidden_continuous_size=16,   \n",
    "    output_size=7,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "# 配置训练器\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=1e-4,\n",
    "            patience=10,\n",
    "            verbose=True,\n",
    "            mode=\"min\"\n",
    "        ),\n",
    "        LearningRateMonitor()\n",
    "    ],\n",
    "    logger=True,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "try:\n",
    "    trainer.fit(\n",
    "        model=tft,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader\n",
    "    )\n",
    "    print(\"\\n模型训练完成\")\n",
    "except Exception as e:\n",
    "    print(f\"训练过程中发生错误:\\n{str(e)}\")\n",
    "    print(f\"PyTorch Lightning 版本: {pl.__version__}\")\n",
    "    print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "    raise\n",
    "\n",
    "# 预测\n",
    "if trainer.is_global_zero:\n",
    "    try:\n",
    "        raw_predictions, x = tft.predict(\n",
    "            val_dataloader,\n",
    "            mode=\"raw\",\n",
    "            return_x=True,\n",
    "            trainer_kwargs={\"accelerator\": \"gpu\" if torch.cuda.is_available() else \"cpu\"}\n",
    "        )\n",
    "        \n",
    "        # 输出预测结果\n",
    "        print(\"\\n预测完成\")\n",
    "        print(f\"预测结果 shape: {raw_predictions['prediction'].shape}\")\n",
    "        print(\"预测时间范围:\",\n",
    "              normalized_df[normalized_df['time_idx'] > training_cutoff]['datetime'].min(),\n",
    "              \"至\",\n",
    "              normalized_df[normalized_df['time_idx'] > training_cutoff]['datetime'].max())\n",
    "    except Exception as e:\n",
    "        print(f\"预测过程中发生错误: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_len = 4\n",
    "encoder_len = 30\n",
    "training_cutoff = normalized_df['datetime'].max() - prediction_len\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    normalized_df[lambda x: x['datetime'] <= training_cutoff],\n",
    "    time_idx='datetime',\n",
    "    target=['solar_output', 'wind_output'],\n",
    "    min_encoder_length=encoder_len,\n",
    "    max_encoder_length=encoder_len,\n",
    "    min_prediction_length=prediction_len,\n",
    "    max_prediction_length=prediction_len,\n",
    "    # 已知的时间变量：时间索引及天气数据（这些数据未来是可知的，如气象预报）\n",
    "    time_varying_known_reals=['datetime', 't2m', 'ws10m', 'ws100m', 'ssrd', 'tp', 'rh'],\n",
    "    # 预测目标是未知变量\n",
    "    time_varying_unknown_reals=['solar_output', 'wind_output'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[],  # 不使用分组\n",
    "        transformation=\"softplus\"\n",
    "    ),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training, normalized_df, predict=True, stop_randomization=True\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(validation, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,           # 隐藏层维度，可根据数据复杂性调整\n",
    "    attention_head_size=1,    # 注意力头数\n",
    "    dropout=0.1,              # Dropout 率，用于防止过拟合\n",
    "    hidden_continuous_size=8, # 连续变量的隐藏层维度\n",
    "    output_size=7,            # 输出层维度，对应QuantileLoss中设定的分位数数量\n",
    "    loss=QuantileLoss(),      # 损失函数\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=1e-4, patience=5, verbose=False, mode=\"min\"\n",
    ")\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop_callback, lr_logger],\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n",
    "\n",
    "raw_predictions, x = tft.predict(val_dataloader, mode=\"raw\", return_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(raw_predictions, x, idx=0):\n",
    "    \"\"\"\n",
    "    绘制预测结果\n",
    "    Args:\n",
    "        raw_predictions: 模型预测的原始结果\n",
    "        x: 输入数据\n",
    "        idx: 要显示的样本索引\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 设置中文字体\n",
    "    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    # 太阳能出力预测\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x[\"encoder_target\"][idx, :, 0], label=\"历史太阳能出力\")\n",
    "    plt.plot(range(encoder_len, encoder_len + prediction_len),\n",
    "             raw_predictions[\"prediction\"][idx, :, 0, 3],  # 中位数预测\n",
    "             label=\"预测太阳能出力\")\n",
    "    plt.title(\"太阳能出力预测\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # 风电出力预测\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x[\"encoder_target\"][idx, :, 1], label=\"历史风电出力\")\n",
    "    plt.plot(range(encoder_len, encoder_len + prediction_len),\n",
    "             raw_predictions[\"prediction\"][idx, :, 1, 3],  # 中位数预测\n",
    "             label=\"预测风电出力\")\n",
    "    plt.title(\"风电出力预测\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 绘制预测结果\n",
    "plot_predictions(raw_predictions, x)\n",
    "\n",
    "# 计算预测误差\n",
    "def calculate_metrics(predictions, actuals):\n",
    "    \"\"\"计算预测指标\"\"\"\n",
    "    mse = np.mean((predictions - actuals) ** 2)\n",
    "    mae = np.mean(np.abs(predictions - actuals))\n",
    "    return mse, mae\n",
    "\n",
    "# 分别计算太阳能和风电的预测误差\n",
    "solar_predictions = raw_predictions[\"prediction\"][:, :, 0, 3]  # 太阳能预测（中位数）\n",
    "wind_predictions = raw_predictions[\"prediction\"][:, :, 1, 3]   # 风电预测（中位数）\n",
    "solar_actuals = x[\"decoder_target\"][:, :, 0]\n",
    "wind_actuals = x[\"decoder_target\"][:, :, 1]\n",
    "\n",
    "solar_mse, solar_mae = calculate_metrics(solar_predictions, solar_actuals)\n",
    "wind_mse, wind_mae = calculate_metrics(wind_predictions, wind_actuals)\n",
    "\n",
    "print(\"\\n预测效果评估:\")\n",
    "print(f\"太阳能出力 - MSE: {solar_mse:.4f}, MAE: {solar_mae:.4f}\")\n",
    "print(f\"风电出力 - MSE: {wind_mse:.4f}, MAE: {wind_mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
